{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1730023f-bc94-43f9-a9ab-e5e331e4bb49",
   "metadata": {},
   "source": [
    "We will now implement SwinUNETR, a transformer-based segmentation model that has shown superior performance in medical image segmentation tasks, especially in 3D datasets.\n",
    "\n",
    "ðŸ“Œ Why SwinUNETR?\n",
    "âœ… Uses Swin Transformer as an encoder for long-range spatial dependencies\n",
    "âœ… Outperforms CNN-based models like U-Net on small datasets\n",
    "âœ… Works well for multi-scale feature extraction in 3D medical images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4ffd4c-bb95-42fa-8fb5-4c89c6a974b2",
   "metadata": {},
   "source": [
    "ðŸ”· Workflow Overview\n",
    "Similar to nnUNet, the workflow includes:\n",
    "\n",
    "Preprocessing (Cropping, Normalization, Augmentation)\n",
    "Manual Annotation (3D Slicer)\n",
    "Human-in-the-Loop Approach\n",
    "Model Training (SwinUNETR)\n",
    "Evaluation & Deployment\n",
    "\n",
    "This is the step-by-step code that you can execute in a Jupyter Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5599779-fb7c-4033-a64a-c83a8b3c6530",
   "metadata": {},
   "source": [
    "ðŸ”· Step 1: Preprocessing\n",
    "Since you've already cropped the laryngeal region and converted .nrrd â†’ .nii.gz, we continue with data normalization and augmentation.\n",
    "\n",
    "ðŸ”¹ 1.1 Install MONAI\n",
    "SwinUNETR is implemented in MONAI, which is built on PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b72d6b-2e51-4f6d-bdb4-986e0f8d47a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pynrrd \n",
    "!pip installnibabel \n",
    "!pip installnumpy\n",
    "!pip install monai\n",
    "!pip install torch\n",
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df77dbb2-3ad4-4db8-b9ba-fe73546e6867",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert .nrrd to .nii.gz\n",
    "import nrrd\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define input and output directories\n",
    "input_folder = \"/path/to/nrrd/\"\n",
    "output_folder = \"/path/to/nifti/\"\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for file in os.listdir(input_folder):\n",
    "    if file.endswith(\".nrrd\"):\n",
    "        file_path = os.path.join(input_folder, file)\n",
    "        data, header = nrrd.read(file_path)  # Read NRRD file\n",
    "        \n",
    "        # Convert to NIfTI format\n",
    "        nifti_img = nib.Nifti1Image(data, affine=np.eye(4))\n",
    "        \n",
    "        # Save the converted file\n",
    "        output_path = os.path.join(output_folder, file.replace(\".nrrd\", \".nii.gz\"))\n",
    "        nib.save(nifti_img, output_path)\n",
    "        print(f\"Converted: {file} â†’ {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066e9bc7-1d22-42a2-8574-7f11708c0fd9",
   "metadata": {},
   "source": [
    "âœ… Output: All nrrd files are converted to .nii.gz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a314bf2-04aa-4171-a28b-77cfce913550",
   "metadata": {},
   "source": [
    "Crop Laryngeal Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d12cbe-58a0-4b49-8153-1cfe1a0b832f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install SimpleITK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1233513-35f3-4130-a350-eaa2eedb66a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cropping Code\n",
    "import SimpleITK as sitk\n",
    "import os\n",
    "\n",
    "input_folder = \"/path/to/nifti/\"\n",
    "output_folder = \"/path/to/cropped_nifti/\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for file in os.listdir(input_folder):\n",
    "    if file.endswith(\".nii.gz\"):\n",
    "        img_path = os.path.join(input_folder, file)\n",
    "        \n",
    "        # Read the image\n",
    "        img = sitk.ReadImage(img_path)\n",
    "        array = sitk.GetArrayFromImage(img)\n",
    "\n",
    "        # Define crop range (adjust based on dataset)\n",
    "        crop_slices = (50, 150)  # Adjust Z-axis range\n",
    "        cropped_array = array[crop_slices[0]:crop_slices[1], :, :]\n",
    "        \n",
    "        # Convert back to NIfTI\n",
    "        cropped_img = sitk.GetImageFromArray(cropped_array)\n",
    "        cropped_img.SetSpacing(img.GetSpacing())\n",
    "        cropped_img.SetDirection(img.GetDirection())\n",
    "        cropped_img.SetOrigin(img.GetOrigin())\n",
    "\n",
    "        # Save cropped image\n",
    "        output_path = os.path.join(output_folder, file)\n",
    "        sitk.WriteImage(cropped_img, output_path)\n",
    "        print(f\"Cropped and saved: {file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90be2d13-571a-4891-b17b-36dbe8087bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize Images\n",
    "\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "input_folder = \"/path/to/cropped_nifti/\"\n",
    "output_folder = \"/path/to/normalized_nifti/\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for file in os.listdir(input_folder):\n",
    "    if file.endswith(\".nii.gz\"):\n",
    "        img_path = os.path.join(input_folder, file)\n",
    "        \n",
    "        # Load image\n",
    "        img = nib.load(img_path)\n",
    "        img_data = img.get_fdata()\n",
    "        \n",
    "        # Normalize: Convert HU range (-1000 to 1000) â†’ [0,1]\n",
    "        img_data = np.clip(img_data, -1000, 1000)\n",
    "        img_data = (img_data + 1000) / 2000  # Normalize to [0,1]\n",
    "        \n",
    "        # Save normalized image\n",
    "        normalized_img = nib.Nifti1Image(img_data, img.affine)\n",
    "        output_path = os.path.join(output_folder, file)\n",
    "        nib.save(normalized_img, output_path)\n",
    "        print(f\"Normalized: {file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64b82f5-0bc9-48c9-a99b-3225ced6871e",
   "metadata": {},
   "source": [
    "âœ… Output: Normalized CT images scaled to [0,1]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7122ca-007b-4d87-953c-5790c157c74b",
   "metadata": {},
   "source": [
    "ðŸ”· Step 2: Annotation Using 3D Slicer\n",
    "âœ… Manually segment 50-100 cases in 3D Slicer\n",
    "âœ… Export segmentations as .nii.gz files\n",
    "\n",
    "ðŸ”¹ Steps\n",
    "Open 3D Slicer and load the cropped, normalized CT scans.\n",
    "Use Segment Editor to manually label the thyroid cartilage.\n",
    "Save as .nii.gz in /path/to/labels/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8394a76f-4ec7-4c7b-a6e9-6f832de9cf65",
   "metadata": {},
   "source": [
    "ðŸ”· Step 3: Human-in-the-Loop (HITL) Approach\n",
    "We train an initial SwinUNETR model on manually labeled images and use it to generate pseudo-labels.\n",
    "\n",
    "ðŸ”¹ Steps\n",
    "Train initial SwinUNETR on manually annotated 50-100 cases.\n",
    "Use the trained model to predict segmentations on unlabeled data.\n",
    "Refine pseudo-labels manually using 3D Slicer.\n",
    "Retrain SwinUNETR on the refined dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7d0746-4f5f-4041-8424-9e3d286bd8b0",
   "metadata": {},
   "source": [
    "ðŸ”· Step 4: SwinUNETR Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9624a64-7e5d-40df-9d40-dcceddca91b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Required Libraries\n",
    "import os\n",
    "import torch\n",
    "import monai\n",
    "from monai.networks.nets import SwinUNETR\n",
    "from monai.transforms import (\n",
    "    Compose, LoadImaged, EnsureChannelFirstd, Spacingd,\n",
    "    ScaleIntensityRanged, CropForegroundd, RandCropByPosNegLabeld,\n",
    "    ToTensord\n",
    ")\n",
    "from monai.data import Dataset, DataLoader\n",
    "from monai.losses import DiceLoss\n",
    "from monai.optimizers.lr_scheduler import WarmupCosineSchedule\n",
    "from monai.metrics import DiceMetric\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cb98e2-6403-4825-82b9-50f296cffdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.2 Define Dataset Paths\n",
    "data_dir = \"/path/to/normalized_nifti/\"\n",
    "label_dir = \"/path/to/labels/\"\n",
    "\n",
    "train_images = [os.path.join(data_dir, f) for f in os.listdir(data_dir)]\n",
    "train_labels = [os.path.join(label_dir, f) for f in os.listdir(label_dir)]\n",
    "\n",
    "train_files = [{\"image\": img, \"label\": lbl} for img, lbl in zip(train_images, train_labels)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973e0426-e187-4c13-be6d-388c5cd5422b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.3 Define Data Augmentation & Preprocessing\n",
    "transforms = Compose([\n",
    "    LoadImaged(keys=[\"image\", \"label\"]),\n",
    "    EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "    Spacingd(keys=[\"image\", \"label\"], pixdim=(1.0, 1.0, 1.0), mode=(\"bilinear\", \"nearest\")),\n",
    "    ScaleIntensityRanged(keys=[\"image\"], a_min=0, a_max=1, b_min=0, b_max=1, clip=True),\n",
    "    CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "    RandCropByPosNegLabeld(keys=[\"image\", \"label\"], spatial_size=(96,96,96), pos=1, neg=1, num_samples=4),\n",
    "    ToTensord(keys=[\"image\", \"label\"])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1439e94d-6103-416f-9e24-55645b4864e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.4 Create DataLoader\n",
    "train_dataset = Dataset(data=train_files, transform=transforms)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3292bbe6-fc44-4009-bc3f-7592d014c8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.5 Define SwinUNETR Model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = SwinUNETR(\n",
    "    img_size=(96,96,96),\n",
    "    in_channels=1,\n",
    "    out_channels=2,\n",
    "    feature_size=48,\n",
    "    use_checkpoint=True\n",
    ").to(device)\n",
    "\n",
    "loss_function = DiceLoss(to_onehot_y=True, softmax=True)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "lr_scheduler = WarmupCosineSchedule(optimizer, warmup_iters=10, max_iters=100)\n",
    "dice_metric = DiceMetric(include_background=False, reduction=\"mean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1536f89f-83ac-4073-9d91-f867276e538d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.6 Train SwinUNETR Model\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch_data in train_loader:\n",
    "        inputs, labels = batch_data[\"image\"].to(device), batch_data[\"label\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae10490f-b725-40e4-8ba1-1fe1e5586358",
   "metadata": {},
   "source": [
    "âœ… Model is trained for 50 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52fa519-61df-4add-8ac8-318ecebc36d0",
   "metadata": {},
   "source": [
    "ðŸ”· Step 5: Model Evaluation & Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9500935a-e2a4-4e13-b4b3-78689df62c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.1 Evaluate Model on Test Data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_data in train_loader:\n",
    "        inputs, labels = batch_data[\"image\"].to(device), batch_data[\"label\"].to(device)\n",
    "        outputs = model(inputs)\n",
    "        dice_score = dice_metric(outputs, labels)\n",
    "        print(f\"Dice Score: {dice_score.mean().item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c43a93d-7c04-4444-9d0d-5aa6c7cfd1cd",
   "metadata": {},
   "source": [
    "âœ… Measures model accuracy using Dice score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c131d4-d589-4c1a-b474-db55e440462d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.2 Inference on New Images\n",
    "test_image = \"/path/to/new_ct.nii.gz\"\n",
    "input_data = transforms({\"image\": test_image})\n",
    "input_tensor = input_data[\"image\"].unsqueeze(0).to(device)\n",
    "\n",
    "model.eval()\n",
    "output = model(input_tensor)\n",
    "predicted_mask = torch.argmax(output, dim=1).cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2cccd3-dc2c-47da-8e84-01366c4a39d6",
   "metadata": {},
   "source": [
    "âœ… Generates automatic segmentations on new CT scans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e215f4-6a47-44e6-901a-5b6c84bca137",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
